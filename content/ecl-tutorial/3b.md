[title=How numbers work]
## How numbers work
Welcome to this, uh, side-tutorial. This explains how bits are used to represent full integer numbers. Though, this will only cover unsigned integers (a type that can only have positive values). It's also quite simplified.  
First of all, every number is made of multiple bits. A bit can only have 2 values - 0 or 1, also referred to as "not set" and "set" respectively. Let's consider an 8 bit number - a number made out of 8 bits. An example value would be `00010110`. So, what number does it represent?  
The bits are read from right to left, just like numbers in the decimal system (the least significant digit is on the right). For convinience, let's index the bites from 0, so the one on far right is the 0th bit, the one before it is the 1st bit etc. The number is read by going through the bits in order, and adding 2 to `n`th power for every bit that's set, with `n` being the index of the bit. So in `00010110` the 0th bit is not set and our number is still 0, 1st bit is set so our number is `2^1=2`, 2nd bit is also set so our number is now `2^1 + 2^2 = 6`, 3rd bit is not set so we ignore, 4th bit is set so our number is now `2^1 + 2^2 + 2^4 = 22`, and the rest of the bits is not set so our number stays as 22. (I feel like I'm terrible at explaining things in a foreign language\.\.\.)  
Another example: `10110001` => `2^0 + 2^4 + 2^5 + 2^7 = 177`  
`01111111` => `2^0 + 2^1 + 2^2 + 2^3 + 2^4 + 2^5 + 2^6 = 127`  
  
`2^0 = 1` obviously  
  
For unsigned numbers that have more bits the process is the same, but there are more bits to consider. In ECL, 32bit numbers are used.  
  
There's a good chance that this explanation is terrible and hard to understand, if that's the case try googling a better one (shouldn't be hard to find)  
[Back to the ECL tutorial](#b=ecl-tutorial/&p=4)